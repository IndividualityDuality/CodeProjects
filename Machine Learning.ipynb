{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Basics:\n",
    "\n",
    "#Statistics in Python:\n",
    "#numpy is the package that is best suited to make calculations about data\n",
    "import numpy as np\n",
    "data = [15, 16, 18, 19, 22, 24, 29, 30, 34]\n",
    "import numpy as np\n",
    "\n",
    "data = [15, 16, 18, 19, 22, 24, 29, 30, 34]\n",
    "\n",
    "print(\"mean:\", np.mean(data)) #Calculates mean of the data\n",
    "print(\"median:\", np.median(data)) #Calculates median of the data\n",
    "print(\"50th percentile (median):\", np.percentile(data, 50)) #Calculates 50th percentile of the data\n",
    "print(\"25th percentile:\", np.percentile(data, 25)) #Calculates 25th percentile of the data\n",
    "print(\"75th percentile:\", np.percentile(data, 75)) #Calculates 75th percentile of the data\n",
    "print(\"standard deviation:\", np.std(data)) #Calculates standard deviation of the data\n",
    "print(\"variance:\", np.var(data)) #Calculates the variance of the data\n",
    "\n",
    "\n",
    "#Reading Data With Pandas:\n",
    "#Pandas is best suited for viewing and manipulating data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "\"\"\"\n",
    "df gives the data frame that we are creating and pd.read_csv('') sets the data frame to the\n",
    "data in parentheses. df.head() shows the first five lines of the data frame.\n",
    "\"\"\"\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 6\n",
    "\"\"\"\n",
    "pd.options.display.max_columns = 6 forces python to display six columns, when it may have otherwise\n",
    "displayed more or less. pd.read... just sets our data frame and df.describe shows a numerical breakdown\n",
    "of each of the columns of data.\n",
    "\"\"\"\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "print(df.describe())\n",
    "\n",
    "#Manipulating Data with Pandas\n",
    "col = df['Fare']\n",
    "\"\"\"\n",
    "Selects a single columns and creates a new data frame of just that column\n",
    "\"\"\"\n",
    "print(col)\n",
    "\n",
    "small_df = df[['Age', 'Sex', 'Survived']]\n",
    "\"\"\"\n",
    "You can do the same thing with a few columns rather than just one. This can be useful for organizing\n",
    "at data you think should be part of a regression.\n",
    "\"\"\"\n",
    "print(small_df.head())\n",
    "\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "\"\"\"\n",
    "You can create a column based on a boolean of a different column. The male column is a boolean\n",
    "based off the gender column.\n",
    "\"\"\"\n",
    "print(df['male'].head())\n",
    "\n",
    "#Numpy Basics:\n",
    "\n",
    "df['Fare']\n",
    "df['Fare'].values\n",
    "\"\"\"\n",
    "Adding \".values\" gives you the specified data as a numpy array\n",
    "\"\"\"\n",
    "\n",
    "df[['Pclass', 'Fare', 'Age']].values \n",
    "\"\"\"\n",
    "\".values\" also works with multiple columns\n",
    "\"\"\"\n",
    "\n",
    "arr = df[['Pclass', 'Fare', 'Age']].values\n",
    "\"\"\"\n",
    "If you create an array of values, you can see the shape of that array with \".shape\"\n",
    "\"\"\"\n",
    "print(arr.shape)\n",
    "\n",
    "arr[0, 1]\n",
    "print(arr[0])\n",
    "print(arr[:,2])\n",
    "\"\"\"\n",
    "The first line of code will give you a specific row and specific column of one data piece\n",
    "The second line will give you the entire row that you specify\n",
    "The last line will give you the value of the second column for all 887 rows\n",
    "\"\"\"\n",
    "\n",
    "#More with Numpy Arrays:\n",
    "mask = arr[:, 2] < 18\n",
    "\"\"\"\n",
    "This creates an array of boolean values. In this case if the person on the Titanic is over\n",
    "or under the age of 18 (adult or child).\n",
    "\"\"\"\n",
    "arr[mask]\n",
    "\"\"\"\n",
    "This will then take the previously defined array but only show the rows in the array for\n",
    "which the boolean value is true (only show the rows the are children).\n",
    "\"\"\"\n",
    "arr[arr[:, 2] < 18] \n",
    "\"\"\"\n",
    "You don't have to create the mask and can instead just plug what you would plug in for mask\n",
    "right into the brackets to filter effectively.\n",
    "\"\"\"\n",
    "print(mask.sum())\n",
    "\"\"\"\n",
    "This tells us how many of the passegers are children by summing up all of the rows that \n",
    "have a boolean value of one.\n",
    "\"\"\"\n",
    "\n",
    "#Plotting Basics:\n",
    "#We almost always use matplotlib.pyplot to do simple plots of data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df['Age'], df['Fare'])\n",
    "\"\"\"\n",
    "This will create a scatter plot with the age as the x-axis and fare as the y-axis.\n",
    "\"\"\"\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "\"\"\"\n",
    "This acually puts labels on the axes\n",
    "\"\"\"\n",
    "\n",
    "plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])\n",
    "\"\"\"\n",
    "The class that the riders have is defined as the parameter. The riders can be in first,\n",
    "second, or third class, which are denoted by the three colors in the plot.\n",
    "\"\"\"\n",
    "plt.plot([0, 80], [85, 5])\n",
    "\"\"\"\n",
    "This will plot a best fit line using data in the specified x-values (0 to 80) and\n",
    "specified y-values (85 to 5).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Classification:\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\"\"\"\n",
    "We set up the data so that it is completely numerical so that we can run a regression on it.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "print(X)\n",
    "print(y)\n",
    "\"\"\"\n",
    "Just checkin the work\n",
    "\"\"\"\n",
    "\n",
    "#Logistic Regression with SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\"\"\"\n",
    "Identify the model that we want to use\n",
    "\"\"\"\n",
    "\n",
    "X = df[['Fare', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "model.fit(X, y)\n",
    "\"\"\"\n",
    "Setting up. Define our variables and input them into the model we want to use\n",
    "\"\"\"\n",
    "\n",
    "print(model.coef_, model.intercept_)\n",
    "\"\"\"\n",
    "Setup the best fit line\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "X = df[['Fare', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_, model.intercept_)#0 = 0.0161594x + -0.01549065y + -0.51037152f\n",
    "\"\"\"\n",
    "Run the regression. The last line will give us the coefficients we can use for our best fit line.\n",
    "This one is pretty simple so you probably would want to run it with more variables in the X Array.\n",
    "\"\"\"\n",
    "\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "model.predict(X)\n",
    "\"\"\"\n",
    "Setting up the model to predict given inputs for the variables in array X. Below gives a prediction as to\n",
    "whether they survive when these conditions apply to that person\n",
    "\"\"\"\n",
    "print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))\n",
    "\n",
    "print(model.predict(X[:5]))\n",
    "\"\"\"\n",
    "Above you predict the outcome of the first five rows using the prediction model. Below you print what the\n",
    "results actually were\n",
    "\"\"\"\n",
    "print(y[:5])\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print((y == y_pred).sum())\n",
    "print((y == y_pred).sum() / y.shape[0])\n",
    "print(model.score(X, y))\n",
    "\"\"\"\n",
    "First you set up your prediction model, then in the second line you set up a boolean that counts all \n",
    "of the times that your prediction matched the reality. The third line calculates for what proportion of the\n",
    "data does the predictor predict correctly.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])\n",
    "df['target'] = cancer_data['target']\n",
    "print(df.head())\n",
    "\n",
    "X = df[cancer_data.feature_names].values\n",
    "y = df['target'].values\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\"\"\"\n",
    "model.fit(X,y) returned a convergence warning that the maximum number of iterations was reached so we\n",
    "changed the solver to liblinear.\n",
    "\"\"\"\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X, y) \n",
    "\n",
    "print(\"prediction for datapoint 0:\", model.predict([X[0]]))\n",
    "print(model.score(X, y))\n",
    "\"\"\"\n",
    "model.score tells you what proportion of the predictions were correct\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Model Evaluation:\n",
    "#Calculating Metrics in SKLearn:\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"accuracy:\", accuracy_score(y, y_pred)) #Accuracy: What percent of the models predictions are correct\n",
    "print(\"precision:\", precision_score(y, y_pred)) #Precision: What percent of the models positive predictions are correct\n",
    "print(\"recall:\", recall_score(y, y_pred)) #Recall: What percent of the positive cases the model predicted correctly\n",
    "print(\"f1 score:\", f1_score(y, y_pred)) #F1 Score: The average of the precision and recall\n",
    "\n",
    "#Confusion Matrix in SKLearn:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, y_pred))\n",
    "\n",
    "#Training and Testing:\n",
    "\n",
    "#Overfitting- When you create a model that performs better than it really would because it is performing on\n",
    "#data that you know the outcome of.\n",
    "#Test Set and Training Set- You normally want to take your data and use 70-80% of it as a training set\n",
    "#to create your model and then use that model on the other 20-30% of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\"\"\"\n",
    "First you have to import train_test_split from SKLearn and then you can split X and y into a train and test\n",
    "case usnig the above line. Below just shows how the data is split.\n",
    "\"\"\"\n",
    "print(\"whole dataset:\", X.shape, y.shape)\n",
    "print(\"training set:\", X_train.shape, y_train.shape)\n",
    "print(\"test set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "#Now that we know about training and test sets, we want to build model based on the training set data\n",
    "#and evaluate the model based on how the model does on the test set.\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "print(\"f1 score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "#ROC Curve:\n",
    "\n",
    "#Sensitivity vs. Specificity- Sensitivity is another word for recal which is \n",
    "#P(true positive)/[p(true positive) + P(false negatives)]. Specificity is the true negative rate which can be calculated\n",
    "#with the formula P(true negative)/[P(true negative) + P(false positive)]\n",
    "\n",
    "#You can use SKLearn to calculate sensitivity and specificity of a prediction model for you. It should be\n",
    "#noted that sensitivity and specificity can only be calculated when you know what the actual outcome is.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "sensitivity_score = recall_score\n",
    "print(sensitivity_score(y_test, y_pred)) \n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(precision_recall_fscore_support(y, y_pred))\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "    return r[0]\n",
    "print(specificity_score(y_test, y_pred)) \n",
    "\n",
    "print(\"sensitivity:\", sensitivity_score(y_test, y_pred))\n",
    "print(\"specificity:\", specificity_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Adjusting the Logistic Regression Threshold in SKLearn\n",
    "#SKLearn naturally finds the probability of an outcome and then rounds it to zero or one so you can see\n",
    "#if the outcome is positive or negative. We can have SKLearn show us the probability it calculated instead of\n",
    "#a zero or one.\n",
    "model.predict_proba(X_test)\n",
    "\"\"\"\n",
    "Above will show us the probabilities of each person surviving or dying on the Titanic. Below we just filtered\n",
    "for one column because once we know one column we know the other.\n",
    "\"\"\"\n",
    "model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred = model.predict_proba(X_test)[:, 1] > 0.75\n",
    "\"\"\"\n",
    "This threshold makes it harder for SKLearn to predict a positive outcome for surviving but we then can\n",
    "feel more confident that the prediction is correct.\n",
    "\"\"\"\n",
    "\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "\n",
    "#ROC Curve:\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('1 - specificity')\n",
    "plt.ylabel('sensitivity')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "The closer the ROC curve is to the left corner, the better the model. Whatever specificity and sensitivity\n",
    "correspond to the point closest to the left corner are the specifity and sensitivity that you should use\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred_proba1 = model1.predict_proba(X_test)\n",
    "print(\"model 1 AUC score:\", roc_auc_score(y_test, y_pred_proba1[:, 1]))\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train[:, 0:2], y_train)\n",
    "y_pred_proba2 = model2.predict_proba(X_test[:, 0:2])\n",
    "print(\"model 1 AUC score:\", roc_auc_score(y_test, y_pred_proba2[:, 1]))\n",
    "\"\"\"\n",
    "AUC (Area Under Curve) is an empirical way to determine which model is better. After creating an ROC curve\n",
    "you use the roc_auc_score function to see the area under each function. The greater the number the better.\n",
    "This test lets us know how effective a logistic regression is for creating a predictive model for this data.\n",
    "\"\"\"\n",
    "#K-Fold Cross Validation:\n",
    "#Multiple Training and Test Sets- When we have a small data set, the random assignment of data to a training\n",
    "#set and a testing set can lead to differences in results if we only run the test once. To combat this, we\n",
    "#can split the data into multiple chunks, lets say 5. You assign one chunk as a test set and the others as\n",
    "#a training set. You create a model with this arrangement of test and training sets, and then switch the \n",
    "#sets around until each of the chunks has been a test set, taking the measures of accuracy each time we \n",
    "#create a new model and averaging the five results afterwards.\n",
    "\n",
    "#K-Fold Cross Validation in SKLearn:\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "y = df['Survived'].values\n",
    "\"\"\"\n",
    "Above you set up your k-fold cross validation by importing the data, setting your variables, and finally\n",
    "priming the k-fold program with the necessary conditions. Below you define how your data will be split into \n",
    "train and test groups and print them to make sure they are right. Lastly, you want to define the train and test\n",
    "groups for x and y so you can run the k-fold cross validation.\n",
    "\"\"\"\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "splits = list(kf.split(X))\n",
    "train_indices, test_indices = splits[0]\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "\n",
    "scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "print(scores)\n",
    "print(np.mean(scores))\n",
    "\"\"\"\n",
    "Now we run the regression with five different splits of data, which is the k-fold cross validation.\n",
    "It takes the five assortments of the data and finds the accuracy of each assortment, finally finding the mean\n",
    "accuracy of these assortments to produce a more correct accuracy score. We now go back to using one model on\n",
    "the whole data set instead of 5 different models, but the accuracy score is still the one we got when we ran\n",
    "the K-fold cross validation.\n",
    "\"\"\"\n",
    "\n",
    "final_model = LogisticRegression()\n",
    "final_model.fit(X, y)\n",
    "\n",
    "#Model Comparison:\n",
    "#You make a few different models and then use K-Fold Cross Validation for each model and compare them to see\n",
    "#which is best\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')\n",
    "df['male'] = df['Sex'] == 'male'\n",
    "\"\"\"\n",
    "Above you just import the programs you need and read in the data set. Below you set up your k-Fold cross\n",
    "validation to be used on the models.\n",
    "\"\"\"\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values\n",
    "X2 = df[['Pclass', 'male', 'Age']].values\n",
    "X3 = df[['Fare', 'Age']].values\n",
    "y = df['Survived'].values\n",
    "\"\"\"\n",
    "You have 3 different x data sets because you want to see which one is best at predicting. You only have\n",
    "one y data set because you want to see which x data set is best at predicting the same y data set.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Logistic Regression with all features\")\n",
    "score_model(X1, y, kf)\n",
    "print()\n",
    "print(\"Logistic Regression with Pclass, Sex & Age features\")\n",
    "score_model(X2, y, kf)\n",
    "print()\n",
    "print(\"Logistic Regression with Fare & Age features\")\n",
    "score_model(X3, y, kf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Decision Tree Model:\n",
    "#What is a Decision Tree?:\n",
    "\"\"\"\n",
    "So far we've only used logistic regression which is parametric, meaning that model it creates is based on \n",
    "defined parameters associated with coefficients. Decision trees are nonparametric so they won't be defined\n",
    "by a set of parameters.\n",
    "\"\"\"\n",
    "\n",
    "#How to Build a Decision Tree:\n",
    "\"\"\"\n",
    "First you have to determine every possible split and then find which ones have the highest score. This\n",
    "score is called the informational gain. This score is between 0 and 1 where one is a perfect split. You\n",
    "ideally want to have all of one outcome on one side of the split and all of the other outcome on the other\n",
    "outcome of of the split.\n",
    "\"\"\"\n",
    "\n",
    "#Gini Impurity: \n",
    "\"\"\"\n",
    "Measures how pure a data set is. A score of .5 means that the data is split exaclty 50-50\n",
    "and a score of 0 or 1 means the data is all in the same class. A higher gini impurity score means that the\n",
    "split you chose split the data more purely. GINI = 2(p)(1-p)\n",
    "\"\"\"\n",
    "\n",
    "#Entropy:\n",
    "\"\"\"\n",
    "Another measurement of the purity of the data set. It has a value between 0 and 1 where a 1\n",
    "means that the data is completely impure (50-50 split) and a 0 means the data is pure (all the same class).\n",
    "ENTROPY = -[plog2p + (1-p)log2(1-p)] <-- log base 2 of p or log base 2 of 1-p\n",
    "\"\"\"\n",
    "\n",
    "#Calculating Information Gain: \n",
    "\"\"\"\n",
    "Information gain uses one of the impurity scores from above to calculate how\n",
    "effective the split that you chose was at splitting the data. A higher score is better. You have to do the\n",
    "calculation for each possible split to see which one is best, but the computer can do the calculation for\n",
    "you. For splits of numerical features, you have to try every possible threshold to see which one provides\n",
    "the greates information gain.\n",
    "\n",
    "INFORMATION GAIN = H(S) - (|A|/|S|)H(A) - (|B|/|S|)H(B)    S is the orginal dataset size, A and B are the\n",
    "size of the datasets that you split dataset S into.\n",
    "\"\"\"\n",
    "\n",
    "#Decision Trees in SKLearn:\n",
    "from sklearn.tree import DecisionTreeClassifier #Import Statement\n",
    "model = DecisionTreeClassifier() #Define the model to be used\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)\n",
    "\"\"\"\n",
    "Set our test and train sets and then a random state so we get different results every time\n",
    "\"\"\"\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict([[3, True, 22, 1, 0, 7.25]])) #Input the situation you want to have predicted\n",
    "print(\"accuracy:\", model.score(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"precision:\", precision_score(y_test, y_pred))\n",
    "print(\"recall:\", recall_score(y_test, y_pred))\n",
    "\n",
    "#To increase the ability of the decision tree, you can run a K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "for criterion in ['gini', 'entropy']:\n",
    "    print(\"Decision Tree - {}\".format(criterion))\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        dt = DecisionTreeClassifier(criterion=criterion)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred = dt.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "    print(\"accuracy:\", np.mean(accuracy))\n",
    "    print(\"precision:\", np.mean(precision))\n",
    "    print(\"recall:\", np.mean(recall))\n",
    "    \n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "feature_names = ['Pclass', 'male']\n",
    "X = df[feature_names].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X, y)\n",
    "\n",
    "dot_file = export_graphviz(dt, feature_names=feature_names)\n",
    "graph = graphviz.Source(dot_file)\n",
    "graph.render(filename='tree', format='png', cleanup=True)\n",
    "\"\"\"\n",
    "The tree diagram is saved as a file. To find the file just search teh file name followed by the format.\n",
    "\"\"\"\n",
    "\n",
    "#Overfitting of Tree Diagrams:\n",
    "#Overfitting is when the model works well on the training set but not on the test set.\n",
    "\n",
    "#Pruning:\n",
    "\"\"\"\n",
    "Setting rules to prevent the branches on the tree from getting too specific.\n",
    "    Pre-Pruning: 3 techniques are used to limit the tree growth\n",
    "        1) Max Depth: Limits the height of the tree. If you set it at 3, there will be at most 3 split for each\n",
    "    data point\n",
    "        2) Leaf Size: Don't split a node if the number of samples at that node is under a set threshold.\n",
    "        3) Number of Leaf Nodes: Set a maximum number of possible leaf nodes in the tree\n",
    "    You want to be careful with pruning because too much can make the model less useful. There is also no set\n",
    "    way to go about pruning to get the best tree diagram. Trial and error is a guarantee.\n",
    "\"\"\"\n",
    "\n",
    "#Pruning with SKLearn:\n",
    "dt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=2, max_leaf_nodes=10)\n",
    "\"\"\"\n",
    "You can put the parameters in the parentheses to change how the tree develops. You can use cross validation\n",
    "to find the best values for the parameters.\n",
    "\"\"\"\n",
    "\n",
    "#Grid Search:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [5, 15, 25],\n",
    "    'min_samples_leaf': [1, 3],\n",
    "    'max_leaf_nodes': [10, 20, 35, 50]}\n",
    "\"\"\"\n",
    "We are giving the grid search parameters on which values to compare for each pruning method.\n",
    "\"\"\"\n",
    "\n",
    "gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)\n",
    "\"\"\"\n",
    "You need to put in four parameters for the grid search: the model you are using (dt for data tree classifier),\n",
    "the grid of values you are comparing (param_grid which we just defined), which metric to use to determine the\n",
    "best model(default is accuracy but we are using f1), and lastly how many folds we want to use (5 in this case).\n",
    "\"\"\"\n",
    "gs.fit(X, y)\n",
    "print(\"best params:\", gs.best_params_)\n",
    "print(\"best score:\", gs.best_score_)\n",
    "\"\"\"\n",
    "The grid search finds the best values out of the ones we gave it to test and prints them for us. It should be\n",
    "noted that these may not be the absolute best values.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Random Forest Model:\n",
    "#Random Forests with SKLearn:\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])\n",
    "df['target'] = cancer_data['target']\n",
    "\n",
    "X = df[cancer_data.feature_names].values\n",
    "y = df['target'].values\n",
    "print('data dimensions', X.shape)\n",
    "\"\"\"\n",
    "Just some standard setup. Import general packages, set data frame, and define variables for model building.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)\n",
    "\"\"\"\n",
    "Now we import the specific packages that we will use for the model and set up the train and test sets along\n",
    "with defining the random state so we get unique results.\n",
    "\"\"\"\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "Define the model that will be used and fit it to the training set.\n",
    "\"\"\"\n",
    "first_row = X_test[0]\n",
    "print(\"prediction:\", rf.predict([first_row]))\n",
    "print(\"true value:\", y_test[0])\n",
    "\"\"\"\n",
    "Quick check to see if it is predicting and check if it's accurate.\n",
    "\"\"\"\n",
    "\n",
    "print(\"random forest accuracy:\", rf.score(X_test, y_test))\n",
    "\"\"\"\n",
    "rf.score(X_test,y_test) takes the rf model that was created for the training set and tries it on the test\n",
    "set. The number produced (0.9790209790209791) is the accuracy of the model (proportion of prediction that\n",
    "are correct). This is above a decision tree model which only has an accuracy score of about 0.9.\n",
    "\"\"\"\n",
    "\n",
    "#Tuning a Random Forest Model:\n",
    "\"\"\"\n",
    "Random forest models are just made up of multiple decision trees, so the prepruning techniques for decision\n",
    "trees (max depth, leaf size, number of leaf nodes) all can be used here too. Two new tuning techniques for\n",
    "random forests are n_estimators which is the number of trees to combine to get a result, and max_features\n",
    "which sets the maximum number of features to be considered at each split. Random forests don't usually\n",
    "require as much tuning as decision trees.\n",
    "\"\"\"\n",
    "rf = RandomForestClassifier(max_features=5)\n",
    "rf = RandomForestClassifier(n_estimators=15)\n",
    "\n",
    "#Grid Search with Random Forests:\n",
    "\"\"\"\n",
    "For deicision trees, we gave a few values for each parameter of the tree for the grid search to compare\n",
    "in order to find the values that will produce the best model. It is very similar with random forests, but\n",
    "there are a few more parameters we can give inputs for.\n",
    "\"\"\"\n",
    "\n",
    " param_grid = {\n",
    "    'n_estimators': [10, 25, 50, 75, 100],\n",
    "}\n",
    " \n",
    "gs = GridSearchCV(rf, param_grid, cv=5)\n",
    "\"\"\"\n",
    "In the decision tree model we used a different metric so we had to specifically change it but for this\n",
    "random forest we are okay with using accuracy.\n",
    "\"\"\"\n",
    "gs.fit(X, y)\n",
    "print(\"best params:\", gs.best_params_)\n",
    "\n",
    "#Elbow Graphs:\n",
    "\"\"\"\n",
    "Adding more trees to the model will never hurt performance but it will make the model more resource\n",
    "intensive. Also, the improvement that comes with adding each tree will taper off. Thankfully, there are\n",
    "elbow graphs to find the sweet spot where you get an effective model without creating a model that is too\n",
    "complicated.\n",
    "\"\"\"\n",
    "\n",
    "n_estimators = list(range(1, 101))\n",
    "param_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "}\n",
    "rf = RandomForestClassifier()\n",
    "gs = GridSearchCV(rf, param_grid, cv=5)\n",
    "gs.fit(X, y)\n",
    "scores = gs.cv_results_['mean_test_score']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = gs.cv_results_['mean_test_score']\n",
    "plt.plot(n_estimators, scores)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0.9, 1)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "If you run the graph you will see that the improvement in accuracy starts to taper off at about ten\n",
    "trees. There is some shifting the graph as you increase the number of trees after 10 but that is probably\n",
    "just due to chance.\n",
    "\"\"\"\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(X, y) \n",
    "\"\"\"\n",
    "Armed with the knowledge that 10 is the optimal number of trees, we can now fit the random forest model\n",
    "to X and y with ten trees and feel good about the quality of the model.\n",
    "\"\"\"\n",
    "\n",
    "#Feature Importances:\n",
    "\"\"\"\n",
    "Some features aren't as important to use when building a model and SKLearn has a package that can show\n",
    "us which features are the most imporant. Only using certain features is helpful because it makes the model\n",
    "less complicated and easier to interpret and explain to others. It can also improve the accuracy of the\n",
    "model as well.\n",
    "\"\"\"\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=111)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "ft_imp = pd.Series(rf.feature_importances_, index=cancer_data.feature_names).sort_values(ascending=False)\n",
    "ft_imp.head(10)\n",
    "\"\"\"\n",
    "We set up our model and then sort the columns by feature importance in descending order, taking a look\n",
    "at the ten most important columns. The columns with \"worst\" seem to be appearing more often as important\n",
    "columns, so below I created a data frame that includes just the columns with \"worst\".\n",
    "\"\"\"\n",
    "worst_cols = [col for col in df.columns if 'worst' in col]\n",
    "print(worst_cols)\n",
    "\n",
    "X_worst = df[worst_cols]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_worst, y, random_state=101)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)\n",
    "\"\"\"\n",
    "Interestingly enough, the accuracy of the random forest including only the features with the word \"worst\"\n",
    "is more accurate than the forest that included all of the features.\n",
    "\"\"\"\n",
    "\n",
    "#Random Forest Pros and Cons:\n",
    "\"\"\"\n",
    "Random forests require less tuning than most other predictive models and work on almost every dataset\n",
    "because they are not trying to fit any line or curve to the data. They are not as interpretable as decision\n",
    "trees because they take multiple smaller decision trees and merge them to make a decision, something that\n",
    "can be difficult to explain in context. They are also a relatively slower than decision trees, but not\n",
    "too slow.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "Neural Networks:\n",
    "#Neurons in Nerual Networks:\n",
    "\"\"\"\n",
    "A neural network is a collection of many nodes that each do small calculations that lead to a \n",
    "meaningful calculation overall. They have inputs of x and outputs of y.\n",
    "\n",
    "Like logistic regression, each neuron uses the general formula w1x1 + w2x2 + b where w1 and w2 are referred\n",
    "to as weights and b is referred to as the bias. The result of this equation is plugged into the activation\n",
    "function as its input. A common activation function is the sigmoid function [sigmoid(x) = 1/(1+e^-x)]. \n",
    "This equation will produce a number between zero and one. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Activation Functions:\n",
    "\"\"\"\n",
    "There are three commonly used activation functions: sigmoid(look above), tanh, and ReLU.\n",
    "\n",
    "tanh(x):\n",
    "    Similar to sigmoid(x) but instead ranges from -1 to 1. \n",
    "    tanh(x) = sinh(x)/cosh(x) = (e^x - e^-x)/(e^x + e^-x)\n",
    "\n",
    "ReLU:\n",
    "    Stands for rectified linear unit\n",
    "    ReLU = {0 if x<=0\n",
    "           {x if x>0\n",
    "\"\"\"\n",
    "\n",
    "#Neural Network:\n",
    "\"\"\"\n",
    "To create a neural network, you combine multiple neurons and use the outputs of some as the inputs of others.\n",
    "One type of neural network is the feed forward multi-layer perceptron. The feed forward means that the\n",
    "neurons only send signals in one direction. Multi-layer perceptron means that it will have an input layer\n",
    "(one node for each input), an output layer (a node for each output) and any number of hidden layers in\n",
    "between that can take multiple inputs from the layers before them and produce an output that can go to \n",
    "multiple other nodes.\n",
    "\n",
    "A single layer perceptron is a neural network without any hidden layers. These are incredibly rare.\n",
    "\n",
    "One of the benefits of neural networks is that they can predict more than two values (0 or 1). It still only\n",
    "produces one value at the end, but it has the ability to produce more than two. An example of this is a\n",
    "program trying to predict what species an animal is. In the past, we would have had to settle for something\n",
    "like cat or not cat, but now we can input a bunch of variables, it will apply those variables to the\n",
    "neural network, and then it will choose from however many species we chose to define.\n",
    "\"\"\"\n",
    "\n",
    "#Training a Neural Network:\n",
    "\"\"\"\n",
    "When trainging a neural network, first you must define a loss function which measure how far from perfect\n",
    "the network is. We train the neural network in an effort to minimize the loss function.\n",
    "\n",
    "Backpropagation: The neural network can work backwards from the output node iteratively improving the\n",
    "loss function each time. Eventually the loss function will be minimized and the optimal function will have\n",
    "been found.\n",
    "\"\"\"\n",
    "\n",
    "#Neural Networks in SKLearn:\n",
    "\"\"\"\n",
    "Sometimes to test the neural network it can be helpful to create an artificial data set. When creating\n",
    "a data set, 5 things have to be set: n_samples(number of datapoints), n_features(number of features),\n",
    "n_informative(number of infomative features), n_redundant(number of redundant features), random_state\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=3)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(X[y==0][:, 0], X[y==0][:, 1], s=100, edgecolors='k')\n",
    "plt.scatter(X[y==1][:, 0], X[y==1][:, 1], s=100, edgecolors='k', marker='^')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "It can be helpful to look at data in a visual way. Remember that this data is random.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "When you run the last 8 lines of code you get a convergence warning. The code shouldn't run and try to \n",
    "optimize forever so there is a set number of iterations that runs but in this case it didn't reach the\n",
    "optimal coefficients for the nodes. To remedy this, increase the maximum number of iterations using the line\n",
    "of code below.\n",
    "\"\"\"\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "\"\"\"\n",
    "You can change some of the parameters of a multi-layer perceptron network such as the number of hidden\n",
    "layers and number of nodes in each layer. The default is a single hidden layer of 100 nodes. Change the\n",
    "numbers below to change the parameters.\n",
    "\"\"\"\n",
    "mlp = MLPClassifier(max_iter=1000, hidden_layer_sizes=(100, 50))\n",
    "\n",
    "\"\"\"\n",
    "Sometimes alpha, the step of the iterations needs to be changed (how much the coefficients change each\n",
    "iteration). Decreasing alpha will make it more likely to reach the optimal solution, but you may also have\n",
    "to increase the number of iterations. You may also want to change the solver, the algorithm that is used\n",
    "to find the optimal solution. The three options are lbfgs, sgd, and adam. One may find the optimal solution\n",
    "faster than another but it is just trial and error.\n",
    "\"\"\"\n",
    "#Predicting Handwritten Digits Example:\n",
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(n_class=2, return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "print(X[0].reshape(8, 8))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(n_class=2, return_X_y=True)\n",
    "plt.matshow(X[0].reshape(8, 8), cmap=plt.cm.gray)\n",
    "plt.xticks(())  # removes x tick marks\n",
    "plt.yticks(())  # removes y tick marks\n",
    "plt.show()\n",
    "\"\"\"\n",
    "The data set is how dark each pixel of an 8x8 picture is. by plotting the data set, we can see the number\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "Above we set up our train and test sets, set up the classifier, and set it up to the data. Below we set x\n",
    "to the first data point in our test set, shape it into an 8x8 array and plot it to see that it is zero.\n",
    "the multi-layer perceptron neural network also predicted that the shape of the plot would be zero.\n",
    "pretty crazy stuff\n",
    "\"\"\"\n",
    "\n",
    "x = X_test[0]\n",
    "plt.matshow(x.reshape(8, 8), cmap=plt.cm.gray)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "print(mlp.predict([x]))\n",
    "\n",
    "x = X_test[1]\n",
    "plt.matshow(x.reshape(8, 8), cmap=plt.cm.gray)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "print(mlp.predict([x]))\n",
    "\"\"\"\n",
    "Correct again\n",
    "\"\"\"\n",
    "\n",
    "print(mlp.score(X_test, y_test))\n",
    "\"\"\"\n",
    "From the score we can see that the neural network had an accuracy of 100%\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Now we are going to try it with all ten digits instead of zero and one\n",
    "\"\"\"\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "mlp = MLPClassifier(random_state=2)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(mlp.score(X_test, y_test))\n",
    "\"\"\"\n",
    "The MLP neural network is \"only\" 96% accurate. Below we figure out where it is getting tripped up.\n",
    "\"\"\"\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "incorrect = X_test[y_pred != y_test]\n",
    "incorrect_true = y_test[y_pred != y_test]\n",
    "incorrect_pred = y_pred[y_pred != y_test]\n",
    "\n",
    "j = 0\n",
    "print(incorrect[j].reshape(8, 8).astype(int))\n",
    "print(\"true value:\", incorrect_true[j])\n",
    "print(\"predicted value:\", incorrect_pred[j])\n",
    "\n",
    "\"\"\"\n",
    "We can see that the first value that the neural network got wrong was when it predicted a 9 when in\n",
    "actuality it was a 4. Understandable.\n",
    "\"\"\"\n",
    "\n",
    "#Visualizing MLP Weights:\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X5 = X[y <= '3']\n",
    "y5 = y[y <= '3']\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(6,), max_iter=200, alpha=1e-4,solver='sgd', random_state=2)\n",
    "mlp.fit(X5, y5)\n",
    "\"\"\"\n",
    "Importing a package, defining variables and the model, and fitting the model to the defined variables.\n",
    "\"\"\"\n",
    "print(mlp.coefs_) #These are the weights for the nodes in the neural network\n",
    "print(len(mlp.coefs_)) #We can see that the list of coefficients has two elements which makes sense; one\n",
    "#is for the hidden layer and one is for the output layer\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(5, 4)) #You can make multiple subplots instead of just oner\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    coef = mlp.coefs_[0][:, i]\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(i + 1)\n",
    "plt.show()\n",
    "\n",
    "#Pros and Cons of Neural Networks:;\n",
    "\"\"\"\n",
    "Neural networks are super complicated and it is impossible to get into the nitty gritty and figure out\n",
    "the network on its most fundamental levels. Because of this, it can feel kind of like putting inputs into \n",
    "a black box and getting outputs that you have to trust. They can also take a pretty long time to train\n",
    "and refine. With all of that said, their performance is extraordinary. They open up unique opportunities by\n",
    "easily tackling what would be a difficult problem for other methods and mostly tune themselves.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
